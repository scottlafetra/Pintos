       	    +---------------------------+
		    |		CS 140		        |
		    | PROJECT 3: VIRTUAL MEMORY	|
		    |	   DESIGN DOCUMENT	    |
		    +---------------------------+

---- GROUP ----

>> Fill in the names and email addresses of your group members.

Scott La Fetra
Alex Riley

---- PRELIMINARIES ----

>> If you have any preliminary comments on your submission, notes for the
>> TAs, or extra credit, please give them here.

>> Please cite any offline or online sources you consulted while
>> preparing your submission, other than the Pintos documentation, course
>> text, lecture notes, and course staff.

			PAGE TABLE MANAGEMENT
			=====================

---- DATA STRUCTURES ----

>> A1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

// Entries for the swap table. Each entry contains a virtual address
// prefix and the physical address it should be mapped to.
struct st_entry
{
  struct hash_elem h_elem;
  struct list_elem l_elem;
  struct sema sema_lock; // To allow multiple reads but not when changing the table
  u_int32 v_addr; // virtual page address
  void* p_addr; // physical page address
}

// "Table" that holds data for eviction. The accessed and dirty bits are accessed
// through the relevant pintos functions, so no actually hash table is necessary
// here.
struct frame_table
{
  struct st_entry* evict_next;
  struct st_entry* clock_ptr; // Used to turn off the access bit periodically
  u_int32 stack_end;
}

// Tracks open and used memory in the swap partition. These are kept in
// separate structs for quick access to the next free entry
struct swap_table
{
  struct list open; // A table of st_entry. Hashed by v_addr
  struct hash used; // A table of st_entry. Hashed by v_addr
}

// Page table entry. Maps virtual addresses to physical addresses
struct pt_entry
{
  struct hash_elem elem;
  u_int32 v_addr;
  u_int32 p_addr;
}

// The page table. Just contains a hash table of elements for translation.
struct page_table
{
  struct list table; // A table of pt_entry. Hashed by v_addr
}

// Supplemental page table entry. Tracks ownership of each page by process id.
struct supt_entry
{
  struct hash_elem elem;
  pid_t pid;
}

// The supplemental page table. Just contains a hash table of page ownership.
struct sup_table
{
  struct list table; // A table of pt_entry. Hashed by v_addr
}


---- ALGORITHMS ----

>> A2: In a few paragraphs, describe your code for locating the frame,
>> if any, that contains the data of a given page.

A user program asks for a page by it's virtual address. This goes to the (virtual) page_table where it is mapped to the correct frame index of the frame_table.
	If the page is on the frame_table we return it.
If the page isn't on the frame_table then we check the swap table.
	If it is on the swap then we allocate a new frame for it on the frame_table, return it's address to the user program, and then free the swap slot. 
	If it isn't on the swap then we go back to the frame_table and we grab the data from the hard drive, allocating a new space for it on the frame_table. We then return it.

>> A3: How does your code coordinate accessed and dirty bits between
>> kernel and user virtual addresses that alias a single frame, or
>> alternatively how do you avoid the issue?

Dirty bits are owned by the kernel virtual page table which the frame table interacts with. The frame table reads these bits from the kernel virtual page table and evicts based on that.

---- SYNCHRONIZATION ----

>> A4: When two user processes both need a new frame at the same time,
>> how are races avoided?

We use a semaphore to allow any amount of threads to read at the same time if no thread is trying to write, but only one thread to have access if we are writing or evicting a frame. If a thread wants to write it is forced to wait until all reading threads have finished reading before it can write. In the meantime no other threads can start reading or writing until the writing thread is done. This protects us from writing at the same time as well as sneaking a read in right before a frame will be evicted. It also allows multiple threads to read simultaneously without locking each other out.

---- RATIONALE ----

>> A5: Why did you choose the data structure(s) that you did for
>> representing virtual-to-physical mappings?

Using arrays would waste memory as they would often be very sparsely populated. Lists would be to slow to index. A tree would definitely be the all around most efficient choice, but the assignment document suggests avoiding them as they add a lot of extra work for not that much efficiency. In the end we decided to store the data in hash maps because they allow indexing while also allowing infinite expandability. 

		       PAGING TO AND FROM DISK
		       =======================

---- DATA STRUCTURES ----

>> B1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

// Entries for the swap table. Each entry contains a virtual address
// prefix and the physical address it should be mapped to.
struct st_entry
{
  struct hash_elem elem;
  struct sema sema_lock; // To allow multiple reads but not when changing the table
  u_int32 v_addr; // virtual page address
  void* p_addr; // physical page address
}

// "Table" that holds data for eviction. The accessed and dirty bits are accessed
// through the relevant pintos functions, so no actually hash table is necessary
// here.
struct frame_table
{
  struct ft_entry* evict_next;
  struct ft_entry* clock_ptr; // Used to turn off the access bit periodically
  u_int32 stack_end;
}

// Tracks open and used memory in the swap partition. These are kept in
// separate structs for quick access to the next free entry
struct swap_table
{
  struct list open; // A table of ft_entry. Hashed by v_addr
  struct hash used; // A table of ft_entry. Hashed by v_addr
}

---- ALGORITHMS ----

>> B2: When a frame is required but none is free, some frame must be
>> evicted.  Describe your code for choosing a frame to evict.

We are using the clock method for eviction. We keep track of two pointers to frames evict_next and clock_ptr. Whenever we a new frame but we are out of memory we move the evict_next up until we find a frame that isn't accessed (which we check in the kernel virtual page table). We evict the frame that hasn't been accessed since the clock_ptr passed it. We also move up the clock_ptr at the same time changing all frames we pass over to be non-accessed. 

>> B3: When a process P obtains a frame that was previously used by a
>> process Q, how do you adjust the page table (and any other data
>> structures) to reflect the frame Q no longer has?

The virtual page table will be updated such that Q's virtual address no longer maps to a frame on the frame table. If Q asks to read that frame again it will page fault off the virtual page table and the frame table will go get that resource again (likely from the swap).

>> B4: Explain your heuristic for deciding whether a page fault for an
>> invalid virtual address should cause the stack to be extended into
>> the page that faulted.

invalid addresses should fault on the virtual page table, and thus we shouldn't have to worry about i allocating stack space.

---- SYNCHRONIZATION ----

>> B5: Explain the basics of your VM synchronization design.  In
>> particular, explain how it prevents deadlock.  (Refer to the
>> textbook for an explanation of the necessary conditions for
>> deadlock.)

We only lock on write, and a thread shouldn't do anything else while writing. We also only use one semaphore per frame so there shouldn't be any oppertunity for deadlocks simply by the fact that threads only have one lock at a time. 

>> B6: A page fault in process P can cause another process Q's frame 
>> to be evicted.  How do you ensure that Q cannot access or modify
>> the page during the eviction process?  How do you avoid a race
>> between P evicting Q's frame and Q faulting the page back in?

We use a semaphore to allow any amount of threads to read at the same time if no thread is trying to write, but only one thread to have access if we are writing or evicting a frame. If a thread wants to write it is forced to wait until all reading threads have finished reading before it can write. In the meantime no other threads can start reading or writing until the writing thread is done. This protects us from writing at the same time as well as sneaking a read in right before a frame will be evicted. It also allows multiple threads to read simultaneously without locking each other out.

>> B7: Suppose a page fault in process P causes a page to be read from
>> the file system or swap.  How do you ensure that a second process Q
>> cannot interfere by e.g. attempting to evict the frame while it is
>> still being read in?

We keep frames that are being read separate, and they are only added to the master table after they are fully read in. 

>> B8: Explain how you handle access to paged-out pages that occur
>> during system calls.  Do you use page faults to bring in pages (as
>> in user programs), or do you have a mechanism for "locking" frames
>> into physical memory, or do you use some other design?  How do you
>> gracefully handle attempted accesses to invalid virtual addresses?

We check the frame table, then swap, then all open files open for that process for the frame in question. If that faults then we take the frame in from Disk. Invalid pages are weeded out in the virtual page table as an invalid page will not be listed. In that case we just throw the error an error.

---- RATIONALE ----

>> B9: A single lock for the whole VM system would make
>> synchronization easy, but limit parallelism.  On the other hand,
>> using many locks complicates synchronization and raises the
>> possibility for deadlock but allows for high parallelism.  Explain
>> where your design falls along this continuum and why you chose to
>> design it this way.

We use semaphore locks for each entry. This allows multiple processes
can read the same entry, but requires that a process wait for all
reads to finish before changing the table. We chose this method because
it allowed for the most parallelism without unnecessary locks.

			 MEMORY MAPPED FILES
			 ===================

---- DATA STRUCTURES ----

>> C1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

// Memory map entry.
struct m_map_entry
{
  struct hash_elem elem;
  uint_t p_addr;
}

// A struct for holding a memory mapped file. We keep track of the
// size so when we evict the pages we only write the bits necessary
// to the file.
struct m_maped_file
{
  struct hash used_pages; // A table of pt_entry. Hashed by v_addr
  size_t size; // file size
}

---- ALGORITHMS ----

>> C2: Describe how memory mapped files integrate into your virtual
>> memory subsystem.  Explain how the page fault and eviction
>> processes differ between swap pages and other pages.

Each thread has it's own list of open m_maped_file structs that each contain a hashmap of all of the virtual addresses of the frames it contains. These are fundamentally no different then other user frames when it comes to the frame table.

>> C3: Explain how you determine whether a new file mapping overlaps
>> any existing segment.

Since the assignment specified that we didn't have to deal with multiple threads having the same file open at the same time our plan merely opens a new copy into memory of each file that is asked for regardless of wether or not there already is an instance of that file open. The files will clobber each other on write, but otherwise there shouldn't be any issues.

---- RATIONALE ----

>> C4: Mappings created with "mmap" have similar semantics to those of
>> data demand-paged from executables, except that "mmap" mappings are
>> written back to their original files, not to swap.  This implies
>> that much of their implementation can be shared.  Explain why your
>> implementation either does or does not share much of the code for
>> the two situations.

The frame_table sees both the same when read, and they are stored together. The only difference is in the functions to grab the data from the disk, which are specific to the kind of data (data vs file), and on eviction in which we merely keep track of whether the frame is part of a file in the frame_table and use the correct eviction scheme based on that.

